{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# ConvLSTM Expl: Convolutional LSTM Network Tutorial"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### This notebook demonstrates the use of a convolutional LSTM network."},{"metadata":{},"cell_type":"markdown","source":"This network is used to predict the next frame of an artificially\ngenerated movie which contains moving squares.\n\nWARNING: please run it with KAGGLE. For unknown reasons works better!\nCode based on: https://keras.io/examples/vision/conv_lstm/ "},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers.convolutional import Conv3D, Conv2D\nfrom keras.layers.convolutional_recurrent import ConvLSTM2D\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.normalization import BatchNormalization\nimport numpy as np\nimport pylab as plt","execution_count":110,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{},"cell_type":"markdown","source":"We create a layer which take as input movies of shape `(n_frames, width, height, channels)` and returns a movie of identical shape."},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras.layers import Reshape\n\nseq = Sequential()\n\n'''\nseq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   input_shape=(None, 40, 40, 1),\n                   padding='same', return_sequences=True))\nseq.add(BatchNormalization())\n\nseq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   padding='same', return_sequences=True))\nseq.add(BatchNormalization())\n\nseq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   padding='same', return_sequences=True))\nseq.add(BatchNormalization())\n\nseq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   padding='same', return_sequences=True))\nseq.add(BatchNormalization())\n'''\n\n\nseq.add(ConvLSTM2D(filters=1, kernel_size=(3, 3), input_shape=(None, 40, 40, 1),\n                   padding='same', return_sequences=True))\n#seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n#               activation='sigmoid',\n#               padding='same', data_format='channels_last'))\nseq.compile(loss='binary_crossentropy', optimizer='adadelta')","execution_count":111,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq.summary()","execution_count":112,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. ","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-112-1a9afb82fbd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1252\u001b[0;31m                 \u001b[0;34m'This model has not yet been built. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m                 \u001b[0;34m'Build the model first by calling build() '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m                 \u001b[0;34m'or calling fit() with some data. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. "]}]},{"metadata":{},"cell_type":"markdown","source":"## Artificial data generation"},{"metadata":{},"cell_type":"markdown","source":"Generate movies with 3 to 7 moving squares inside.\n\nThe squares are of shape 1x1 or 2x2 pixels, which move linearly over time.\n\nFor convenience we first create movies with bigger width and height (80x80) and at the end we select a 40x40 window."},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_movies(n_samples=1200, n_frames=15):\n    row = 80\n    col = 80\n    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1),\n                              dtype=np.float)\n\n    for i in range(n_samples):\n        # Add 3 to 7 moving squares\n        n = np.random.randint(3, 8)\n\n        for j in range(n):\n            # Initial position\n            xstart = np.random.randint(20, 60)\n            ystart = np.random.randint(20, 60)\n            # Direction of motion\n            directionx = np.random.randint(0, 3) - 1\n            directiony = np.random.randint(0, 3) - 1\n\n            # Size of the square\n            w = np.random.randint(2, 4)\n\n            for t in range(n_frames):\n                x_shift = xstart + directionx * t\n                y_shift = ystart + directiony * t\n                noisy_movies[i, t, x_shift - w: x_shift + w,\n                             y_shift - w: y_shift + w, 0] += 1\n\n                # Make it more robust by adding noise.\n                # The idea is that if during inference,\n                # the value of the pixel is not exactly one,\n                # we need to train the network to be robust and still\n                # consider it as a pixel belonging to a square.\n                if np.random.randint(0, 2):\n                    noise_f = (-1)**np.random.randint(0, 2)\n                    noisy_movies[i, t,\n                                 x_shift - w - 1: x_shift + w + 1,\n                                 y_shift - w - 1: y_shift + w + 1,\n                                 0] += noise_f * 0.1\n\n                # Shift the ground truth by 1\n                x_shift = xstart + directionx * (t + 1)\n                y_shift = ystart + directiony * (t + 1)\n                shifted_movies[i, t, x_shift - w: x_shift + w,\n                               y_shift - w: y_shift + w, 0] += 1\n\n    # Cut to a 40x40 window\n    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n    noisy_movies[noisy_movies >= 1] = 1\n    shifted_movies[shifted_movies >= 1] = 1\n    return noisy_movies, shifted_movies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"noisy_movies, shifted_movies = generate_movies(n_samples=1200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`(batch_size, time_steps, height, width, filters (layers)`"},{"metadata":{"trusted":true},"cell_type":"code","source":"noisy_movies.shape, shifted_movies.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the network"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nseq.fit(noisy_movies[:1000], shifted_movies[:1000], batch_size=10,\n        epochs=5, validation_split=0.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing the network on one movie"},{"metadata":{},"cell_type":"markdown","source":"Feed it with the first 7 positions and then predict the new positions"},{"metadata":{"trusted":true},"cell_type":"code","source":"which = 1006\ntrack = noisy_movies[which][:7, ::, ::, ::]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the dimensions of the following array to better understand what's going on here.\n\n`track` has the shape of 7 frames 40\\*40 with one channel. `np.newaxis` adds additional axis so the array can be accepted by the `seq` model."},{"metadata":{"trusted":true},"cell_type":"code","source":"track.shape, track[np.newaxis, ::, ::, ::, ::].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(16):\n    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::]) # (1, 7, 40, 40, 1)\n    new = new_pos[::, -1, ::, ::, ::] # (1, 40, 40, 1)\n    #print(track.shape)\n    #print(new.shape)\n    track = np.concatenate((track, new), axis=0) # adds +1 to the first dimension in each loop cycle","execution_count":113,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Error when checking input: expected sequential_13_input to have shape (23, 40, 40, 1) but got array with shape (24, 40, 40, 1)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-113-38ffbecbb298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnew_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (1, 7, 40, 40, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (1, 40, 40, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(track.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(new.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking input: expected sequential_13_input to have shape (23, 40, 40, 1) but got array with shape (24, 40, 40, 1)"]}]},{"metadata":{},"cell_type":"markdown","source":"## Compare the predictions to the ground truth"},{"metadata":{"trusted":true},"cell_type":"code","source":"track2 = noisy_movies[which][::, ::, ::, ::]\n\nfor i in range(15):\n    fig = plt.figure(figsize=(10, 5))\n\n    ax = fig.add_subplot(121)\n\n    if i >= 7:\n        ax.text(1, 3, 'Predictions !', fontsize=20, color='w')\n    else:\n        ax.text(1, 3, 'Initial trajectory', fontsize=20)\n\n    toplot = track[i, ::, ::, 0]\n\n    plt.imshow(toplot)\n    ax = fig.add_subplot(122)\n    plt.text(1, 3, 'Ground truth', fontsize=20)\n\n    toplot = track2[i, ::, ::, 0]\n    if i >= 2:\n        toplot = shifted_movies[which][i - 1, ::, ::, 0]\n\n    plt.imshow(toplot)\n    plt.savefig('%i_animate.png' % (i + 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}