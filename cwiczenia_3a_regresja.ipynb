{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cwiczenia_3a_regresja.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1wlvyJSGjE8OoIliEdl+D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcinwolter/DeepLearning_2020/blob/main/cwiczenia_3a_regresja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQH7zbVF8v6g"
      },
      "source": [
        "\n",
        "\n",
        "# **A regression example**\n",
        "\n",
        "Common type of machine learning problem is \"regression\", which consists of predicting a continuous value instead of a discrete label. For instance, predicting the temperature tomorrow, given meteorological data, or predicting the time that a software project will take to complete, given its specifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zl5s4feH_v4"
      },
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from keras import optimizers\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvsAzyjTI3Ie"
      },
      "source": [
        "**1-dimensional data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fEyq6reI8Vi"
      },
      "source": [
        "def funct(x):\n",
        "  return x*x*np.sin(x)+np.log(2*x)\n",
        "\n",
        "size = 100\n",
        "low=0\n",
        "high=12\n",
        "error=10\n",
        "\n",
        "XX_train = np.random.uniform(low=low, high=high, size=size)\n",
        "XX_test = np.random.uniform(low=low, high=high, size=size)\n",
        "yy_train = funct(XX_train) + np.random.normal(0., error, size)\n",
        "yy_test = funct(XX_test) + np.random.normal(0., error, size)\n",
        "\n",
        "print(XX_train.shape, yy_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK9f3EkaNLKy"
      },
      "source": [
        "Plot the function and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc4wlpu6NPib"
      },
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "\n",
        "plt.plot(XX_train,yy_train, 'o', color='blue', label='Training points')\n",
        "plt.plot(XX_test,yy_test, 'o', color='green', label='Testing points')\n",
        "\n",
        "points = np.linspace(low, high,num=100)\n",
        "plt.plot(points, funct(points),  color='red', label='Function')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS7hU2fiZHzl"
      },
      "source": [
        "\n",
        "**Preparing the data**\n",
        "\n",
        "It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a unit standard deviation. This is easily done in Numpy:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNzLzYMFZNab"
      },
      "source": [
        "# Tutaj należy znormalizować dane, i.e. \n",
        "# - odjąć średnią\n",
        "# - podzielić przez odchylenie standardowe\n",
        "\n",
        "# \n",
        "mean = XX_train.mean(axis=0)\n",
        "XX_train_n = XX_train-mean\n",
        "std = XX_train_n.std(axis=0)\n",
        "XX_train_n /= std\n",
        "\n",
        "# to samo dla X_test\n",
        "XX_test_n = XX_test - mean\n",
        "XX_test_n /= std\n",
        "\n",
        "# A teraz to samo dla yy_train i yy_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1akSHhNWUOI"
      },
      "source": [
        "Define neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKnzCxTvWlLF"
      },
      "source": [
        "# Zbudować sieć neuronową, działa: dwie warstwy ukryte, aktywacja relu\n",
        "\n",
        "model0 = Sequential(name='network')\n",
        "#.....\n",
        "\n",
        "model0.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9pQ61rwWX9F"
      },
      "source": [
        "Train neural network\n",
        "\n",
        "For regression problems, mean squared error (MSE) is often employed\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hos2Vg10XBU3"
      },
      "source": [
        "# compile and train NN\n",
        "# Uwaga: dla regresji używamy: loss = 'mean_squared_error'   oraz metrics = ['mse']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l8g2s4Ogwt7"
      },
      "source": [
        "Plot loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksInevx1gt33"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 0.2])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error ')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_loss(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhWj-M9sWa1A"
      },
      "source": [
        "Evaluate neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAGobBA6XfTt"
      },
      "source": [
        "results = model0.evaluate(XX_test_n, yy_test_n)\n",
        "\n",
        "print('loss test data: ', results[0])\n",
        "print('mse test data: ', results[1])\n",
        "\n",
        "results = model0.evaluate(XX_train_n, yy_train_n)\n",
        "\n",
        "print('loss train data: ', results[0])\n",
        "print('mse train data: ', results[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qv-t09rWgT8"
      },
      "source": [
        "Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwHkHPnzvD_z"
      },
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "\n",
        "plt.plot(XX_train_n,yy_train_n, 'o', color='blue', label='Training points')\n",
        "plt.plot(XX_test_n,yy_test_n, 'o', color='green', label='Testing points')\n",
        "\n",
        "points = np.linspace(min(XX_test_n), max(XX_test_n),num=100)\n",
        "plt.plot(points, (funct(points*std+mean)-meany)/stdy,  color='red', label='Function')\n",
        "\n",
        "plt.plot(points, model0.predict(points),  color='orange', label='Neural net')\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnwcHVfaHJif"
      },
      "source": [
        "**The Boston Housing Price dataset**\n",
        "\n",
        "We will be attempting to predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points about the suburb at the time, such as the crime rate, the local property tax rate, etc.\n",
        "\n",
        "The dataset has very few data points, only 506 in total, split between 404 training samples and 102 test samples, and each \"feature\" in the input data (e.g. the crime rate is a feature) has a different scale. For instance some values are proportions, which take a values between 0 and 1, others take values between 1 and 12, others between 0 and 100..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__lqdBmD9cgL"
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDCcGuFW9RP6"
      },
      "source": [
        "\n",
        "**Preparing the data**\n",
        "\n",
        "It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a unit standard deviation. This is easily done in Numpy:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRPumcs09RP7"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78oNgkuWE6vb"
      },
      "source": [
        "Define, compile and train neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuj5f4Mg6e5y"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86NDw6Qm91dC"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 0.5])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [MPG]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_loss(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2enpb6gFGte"
      },
      "source": [
        "Evaluate neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dewsTB7XbAh"
      },
      "source": [
        "results = model.evaluate(X_test_n, y_test_n)\n",
        "\n",
        "print('loss test data: ', results[0])\n",
        "print('mse test data: ', results[1])\n",
        "\n",
        "results = model.evaluate(X_train_n, y_train_n)\n",
        "\n",
        "print('loss train data: ', results[0])\n",
        "print('mse train data: ', results[1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}